# 字节面试：如何从抖音100亿条用户搜索日志中找出最热门的100个搜索词？





<font style="color:rgb(0, 0, 0);">前段时间，</font><font style="color:rgb(6, 8, 31);">我的一个朋友在字节的面试中，栽在了一道看似简单的大数据处理题上。</font>

<font style="color:rgb(6, 8, 31);">面试官抛出了一个看似简单却暗藏玄机的问题：</font>

假设你是抖音搜索团队的工程师，**<font style="color:rgb(243, 50, 50);">每天需要分析1TB的用户搜索日志，找出最热门的100个搜索词</font>**。具体条件如下：

+ <font style="color:rgb(0, 0, 0);">数据规模：1TB日志文件（约100亿条记录）</font>
+ <font style="color:rgb(0, 0, 0);">内存限制：4GB</font>
+ <font style="color:rgb(0, 0, 0);">要求：找出出现频率最高的100个搜索词</font>
+ <font style="color:rgb(0, 0, 0);">响应时间：尽可能快</font>

<font style="color:rgb(6, 8, 31);">我朋友当时的内心：</font><font style="color:rgb(6, 8, 31);">😱</font>

这是一道<font style="color:rgb(38, 38, 38);">在大数据处理领域非常实用的算法题：</font>**<font style="color:rgb(243, 50, 50);">如何在海量搜索词中找出TOP100热词</font>****。**这个问题在实际应用中非常常见，无论是搜索引擎优化、社交媒体趋势分析，还是电商平台的商品推荐，都离不开这个技术。

## **大数据处理的“杀手锏”：分而治之**
面对海量数据，我们需要一个既高效又省内存的方案。这里，我们采用“分而治之”的策略：

### **1. 数据分片**
将1TB的大文件智能地切分成多个小文件<font style="color:rgb(6, 8, 31);">，每个文件保持在 </font>**<font style="color:rgb(6, 8, 31);">几百MB</font>**<font style="color:rgb(6, 8, 31);"> 的大小。这是为了确保每个分片在处理时不会超出内存限制。</font>

```plain
def split_large_file(file_path, chunk_size=100*1024*1024):  # 100MB  
    chunks = []  
    with open(file_path, 'r') as f:  
        chunk = []  
        for line in f:  
            chunk.append(line)  
            if len(chunk) * len(line) >= chunk_size:  
                chunks.append(chunk)  
                chunk = []  
        
        if chunk:  # 处理最后一个不完整的chunk  
            chunks.append(chunk)  
    
    return chunks  
```

### **2. 分片处理**
<font style="color:rgb(6, 8, 31);">对每个小文件进行并行词频统计。通过限制每片的大小，确保我们在任何时间点都不会超出内存限制，就像多个工人同时搬砖，效率倍增！</font>

```plain
def count_word_frequency(chunk):  
    word_counts = {}  
    for line in chunk:  
        word = line.strip()  
        word_counts[word] = word_counts.get(word, 0) + 1  
    return word_counts  
```

### **3. 结果合并**
<font style="color:rgb(6, 8, 31);">使用小顶堆快速合并各个分片的结果，找出全局 </font>**<font style="color:rgb(6, 8, 31);">TOP 100</font>**<font style="color:rgb(6, 8, 31);">。小顶堆的使用使得我们能够在内存有限的情况下，高效地保留最高频的词。</font>

```plain
import heapq  

def merge_top_k(partition_results, k=100):  
    global_heap = []  
    
    for result in partition_results:  
        for word, count in result.items():  
            if len(global_heap) < k:  
                heapq.heappush(global_heap, (count, word))  
            elif count > global_heap[0][0]:  
                heapq.heapreplace(global_heap, (count, word))  
    
    return sorted(global_heap, reverse=True)  
```

## **核心技术解密**
这个方案有三大杀手锏：

1. <font style="color:rgb(0, 0, 0);">哈希分区：将数据均匀分布到不同的文件中，避免单个文件过大。</font>
2. <font style="color:rgb(0, 0, 0);">并行计算：充分利用多核CPU，通过多线程/多进程并行处理分片，提高效率。</font>
3. <font style="color:rgb(0, 0, 0);">小顶堆：在内存有限的情况下高效筛选出Top K（本例中为TOP 100）。</font>

大数据处理看似复杂，其实是“分治”思想的完美体现。关键在于：

+ <font style="color:rgb(0, 0, 0);">将大问题拆解成小问题</font>
+ <font style="color:rgb(0, 0, 0);">严格控制内存使用</font>
+ <font style="color:rgb(0, 0, 0);">高效合并结果</font>

在抖音这样的超大规模系统中，通常还会：

+ <font style="color:rgb(0, 0, 0);">使用 </font>**<font style="color:rgb(0, 0, 0);">Spark/Flink</font>**<font style="color:rgb(0, 0, 0);"> 进行分布式计算，以进一步扩展处理能力。</font>
+ <font style="color:rgb(0, 0, 0);">采用 </font>**<font style="color:rgb(0, 0, 0);">HDFS</font>**<font style="color:rgb(0, 0, 0);"> 等分布式文件系统，方便存储和处理大规模数据。</font>
+ <font style="color:rgb(0, 0, 0);">引入实时监控和告警机制，确保系统稳定。</font>

<font style="color:rgb(38, 38, 38);">希望今天的分享对大家有所帮助！如果你对这个问题还有其他思路或疑问，欢迎留言讨论哦！</font>



> 更新: 2025-01-02 12:10:56  
> 原文: <https://www.yuque.com/u12222632/as5rgl/lgf6u1phnzqp4u2t>